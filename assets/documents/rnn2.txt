- **Long-Short-Term Memory _Recurrent Neural Network_** belongs to the family of **deep learning algorithms**. It is a recurrent network because of the _feedback connections_ in its architecture. It has an **advantage** over traditional neural networks due to its capability to process the entire sequence of data. Its architecture comprises the cell, input gate, output gate and forget gate. **(SEE BELOW FOR MODEL ARCHITECTURE)**

- The cell **remembers values** over arbitrary time intervals, and the three gates regulate the flow of information into and out of the cell. The cell of the model is responsible for keeping track of the dependencies between the elements in the input sequence. The input gate controls the extent to which a new value flows into the cell, the forget gate controls the extent to which a value remains in the cell, and the output gate controls the extent to which the value in the cell is used to compute the output activation of the LSTM unit.

- However, there are some **variants** of the LSTM model such as **Gated Recurrent Units (GRUs)** that do not have the output gate. **LSTM Networks** are popularly used on **time-series data for classification**, processing, and making predictions. The reason for its popularity in time-series application is that there can be several lags of unknown duration between important events in a time series.